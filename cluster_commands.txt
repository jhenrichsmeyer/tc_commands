######################################################################
# main authors: Mats Simmermacher, Andres Moreno Carrascosa          #
# and Stefan Behnle                                                  # 
######################################################################

Please find your own sensible structure for either different job schedulers that
is understandable for both people in TÃ¼bingen and Edinburgh.

============================================================================
==              Basic Commands for Computing on EDDIE (UoE)               ==
============================================================================

- you can access EDDIE with
ssh <UUN>@eddie.ecdf.ed.ac.uk,
where UUN is your University User Name

- jobs can be submitted with
qsub <script.sh>,
where script.sh is a submission script

- submitted or running jobs can be monitored with
qstat;
this also displays the IDs of the jobs

- submitted or running jobs can be killed with
qdel <job ID>,
where the job ID can be found via qstat

- an examplary submission script that submits a calculation
with a Python script named hello.py looks as follows:
#!/bin/sh
# Grid Engine options (lines prefixed with #$)
#$ -N hello              
#$ -cwd                  
#$ -l h_rt=00:05:00 
#$ -l h_vmem=1G
#  These options are:
#  job name: -N
#  use the current working directory: -cwd
#  runtime limit of 5 minutes: -l h_rt
#  memory limit of 1 Gbyte: -l h_vmem
# Initialise the environment modules
. /etc/profile.d/modules.sh
# Load Python
module load python/3.4.3
# Run the program
./hello.py

============================================================================
=               BASIC CONSIDERATIONS WHEN YOU FIRST ENTER A CLUSTER      =     
============================================================================

- Before running jobs in any cluster, please bear in mind LOGIN NODES are not RUNNING NODES
- Knowing the number of cores and nodes in your cluster is important before submitting your jobs
- The time limit you put in your scripts will determine your position in the queue, do not put 24h to run H2
- Sometimes you will realize a job you just submitted has some error. It is a good practice to kill it before starting a new one. Having a list with your PID's (Process Identifier) also helps in that task if your user is shared
- Also be careful with the memory assignated to each job, an useful practice would be to read the cluster documentation before hand
- In case some information is not in your university webpage, general instructions can be found in the webpages related to your specific queue system
- If el barrilero is on your group, be careful, he is muuuuuuy estupido (although Elke and Andres do not agree on this!) 
(Es el mas estupido. Muchas!!)
- If you use EMACS, please leave science or consider a quick change to vi
- Before running your brand new code in the cluster, please test it on your local machine (not in the login node) 
- Remember to change the libraries for every run, if you do not source the libraries (for example Tensorflow)  in your submitting script, you will not be able to use it later.  
- If you are using a script some person in your group has written, please please please, change the email so he/she does not reseive an email every time you run something
- ENJOY THE FUNNY AND WELL PAID JOB OF BEING A COMPUTATIONAL SCIENTIST!  
MUCHAS!!!
=======

(*) There are several ways to find out the process ID (PID) of a rumming process:
- htop has a column that lists the PID
- top can show you the PID
- ps shows you your processes and the related PID
- graphical process managers/task managers also list PIDs

==============================================================================
==                    THE  S L U R M  QUEUEING SYSTEM                       ==
==============================================================================

The SLURM workload manager(Simple Linux Utility for Resource Management, https://www.schedmd.com/)
is a free and open-source queueing system.

The general documentation and tutorials are available from https://slurm.schedmd.com/
For a quick overview see https://slurm.schedmd.com/quickstart.html

The most important commands are

* sbatch <name_of_job_script>

  submits the job defined in <name_of_job_script> to the queue


* squeue
  
  list the jobs in the queue


* scancel <jobid>

    deletes the job specified by <jobid>



Ressource allocations and job settings use the #SBATCH specifier. Some useful settings are
#SBATCH -J name_of_the_job                     <- this assigns the name "name_of_the_job" to the job
#SBATCH --time=100:00:00                       <- requests a wall clock time limit of 100 hours
#SBATCH --mem=30G                              <- requests a memory limit of 30 GiB for the job (per node)
#SBATCH --nodes=1                              <- requests one node
#SBATCH --ntasks=1                             <- requests one task
#SBATCH --cpus-per-task=20                     <- requests 20 cpus for the one task

SLURM does not only know nodes and cores but also tasks. 

The queueing system can inform you by sending mails if something unexpected happens or if your job dies/finishes:

#SBATCH --mail-user=username@my-university.com  <- set the mail address used for sending any messages from the queueing system
#SBATCH --mail-type=FAIL                        <- only send mails if the job fails

These ressource allocation specifiers are usually placed directly at the beginning of the script.

so an example script would look like
#!/usr/bin/bash
#SBATCH -J name_of_the_job                     <- this assigns the name "name_of_the_job" to the job
#SBATCH --time=100:00:00                       <- requests a wall clock time limit of 100 hours
#SBATCH --mem=30G                              <- requests a memory limit of 30 GiB for the job (per node)
#SBATCH --nodes=1                              <- requests one node
#SBATCH --ntasks=1                             <- requests one task
#SBATCH --cpus-per-task=20                     <- requests 20 cpus for the one task

module load chem/my_favourite_code

<make local scratch directory>

cd $SLURM_SUBMIT_DIR

myprog inputfile > outputfile

<more stuff>
<clean up scratch>
